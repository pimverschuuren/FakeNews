{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1RQC3WVPqkuXexEyp7xPNF4BUAecji_LB",
      "authorship_tag": "ABX9TyMOiE8YrNliK5pNjBLplefv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pimverschuuren/FakeNews/blob/main/Simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkL5A_Uda1Wp"
      },
      "source": [
        "This notebook implements a recurrent neural network model as a binary classifier that identifies text as fake or true.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K2hi54SWSh6"
      },
      "source": [
        "Create objects that will facilitate the tokenization of the textual data. Only include the columns that contain the text and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blwe0H6RWSH8"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "fields = [(None, None), (None, None), (None, None), ('text', TEXT),('label',LABEL)]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9vgytCrcHXH"
      },
      "source": [
        "Load the csv file into a pytorch dataset object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuVYoBuRBUfU"
      },
      "source": [
        "tab_data = data.TabularDataset(path = 'drive/MyDrive/Colab Notebooks/FakeNews/FakeNews.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX69x2TMce3T"
      },
      "source": [
        "Split the dataset into training, testing and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPTNjVTpE1Xc"
      },
      "source": [
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "train_data, test_data = tab_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
        "train_data, validation_data = train_data.split(split_ratio=0.7, random_state = random.seed(SEED))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdTEiy8Ffw3"
      },
      "source": [
        "Print the lengths of each dataset and an example data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEmKYZm_BYgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04912cb8-2f29-4eab-9063-ab285a9b4adf"
      },
      "source": [
        "print(\"Number of training examples: \"+str(len(train_data)))\n",
        "print(\"Number of validation examples: \"+str(len(validation_data)))\n",
        "print(\"Number of testing examples: \"+str(len(test_data)))\n",
        "print(\"Example data point:\")\n",
        "print(vars(validation_data[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 10192\n",
            "Number of validation examples: 4368\n",
            "Number of testing examples: 6240\n",
            "Example data point:\n",
            "{'text': ['In', 'the', 'battle', 'against', 'the', 'Islamic', 'State', 'for', 'Mosul', ',', 'Iraq', '’s', '  ', 'city', ',', 'there', 'is', 'no', 'continuous', 'front', 'line', ',', 'but', 'a', 'patchwork', 'of', 'battlegrounds', 'in', 'the', 'city', 'and', 'all', 'around', 'its', 'edge', '.', 'When', 'the', 'Islamic', 'State', ',', 'also', 'known', 'as', 'ISIS', 'or', 'ISIL', ',', 'retreats', 'from', 'a', 'position', ',', 'it', 'tries', 'to', 'leave', 'as', 'much', 'damage', 'behind', 'as', 'possible', ',', 'including', 'burning', 'oil', 'field', 'wells', 'to', 'provide', 'concealment', 'ahead', 'of', 'a', 'government', 'advance', '.', 'Snipers', 'are', 'also', 'in', 'place', ',', 'ready', 'to', 'strike', '.', 'A', 'village', 'or', 'neighborhood', 'that', 'is', 'retaken', 'by', 'government', 'troops', 'could', 'soon', 'be', 'flooded', 'again', 'with', 'extremist', 'fighters', ',', 'who', 'are', 'rarely', 'far', 'away', '.', 'I', 'recently', 'spent', 'three', 'weeks', 'in', 'this', 'volatile', 'zone', ',', 'first', 'with', 'the', 'Iraqi', 'Army', 'in', 'eastern', 'Mosul', 'and', 'then', ',', 'after', 'setbacks', 'for', 'the', 'army', ',', 'whose', '   ', 'battle', 'of', 'attrition', 'was', 'not', 'going', 'the', 'way', 'it', 'had', 'hoped', ',', 'with', 'units', 'of', 'the', 'Iraqi', 'federal', 'police', '.', 'The', 'police', ',', 'traveling', 'in', 'armored', 'vehicles', ',', 'are', 'playing', 'a', 'significant', 'role', 'in', 'the', 'battle', 'for', 'Mosul', ',', 'sometimes', 'supporting', 'the', 'army', '.', 'They', 'are', 'also', 'responsible', 'for', 'holding', 'their', 'own', 'territory', 'around', 'the', 'city', '.', 'The', 'police', 'have', 'been', 'tasked', 'with', 'maintaining', 'areas', 'of', 'the', 'front', 'line', 'nearby', ',', 'ISIS', 'fighters', 'are', 'ready', 'and', 'in', 'position', '.', 'When', 'holding', 'ground', 'and', 'not', 'on', 'an', 'advance', ',', 'the', 'two', 'sides', 'intermittently', 'exchange', 'rifle', 'and', 'mortar', 'fire', ',', '  ', 'shows', 'of', 'force', 'that', 'tend', 'to', 'harass', 'more', 'than', 'inflict', 'serious', 'casualties', '.', 'For', 'the', 'two', 'days', 'I', 'was', 'embedded', 'with', 'the', 'police', ',', 'they', 'had', 'been', 'ordered', 'to', 'pause', 'in', 'their', 'advance', '.', 'So', 'for', 'the', 'most', 'part', ',', 'they', 'were', 'left', 'to', 'wait', 'along', 'a', 'muddy', 'front', 'until', 'the', 'next', 'push', 'forward', '.', 'For', 'the', 'civilians', 'trapped', 'by', 'the', 'fighting', ',', 'life', 'is', 'bleak', ',', 'and', 'most', 'are', 'desperate', 'to', 'leave', '.', 'Areas', 'along', 'the', 'contested', 'front', 'have', 'no', 'water', ',', 'no', 'electricity', 'and', 'no', 'way', 'for', 'people', 'to', 'obtain', 'basic', 'supplies', '.', 'Those', 'trapped', 'by', 'the', 'fighting', 'and', 'those', 'who', 'return', 'hoping', 'to', 'restart', 'their', 'lives', ' ', '—', '  ', 'or', 'at', 'least', 'to', 'recover', 'some', 'belongings', ' ', '—', '  ', 'are', 'at', 'risk', 'of', 'being', 'caught', 'in', 'the', '  ', 'and', 'injuries', 'and', 'deaths', 'among', 'civilians', 'are', 'increasing', '.', 'When', 'government', 'forces', 'liberate', 'an', 'area', ',', 'most', 'civilians', 'act', 'quickly', 'to', 'try', 'to', 'evacuate', '.', 'However', ',', 'the', 'Islamic', 'State', 'appears', 'eager', 'to', 'keep', 'as', 'many', 'civilians', 'as', 'possible', 'in', 'the', 'urban', 'center', ',', 'using', 'them', 'as', 'shields', 'against', 'aggressive', 'bombing', 'campaigns', 'by', 'the', 'government', 'and', 'coalition', 'forces', '.', 'Anyone', 'seen', 'fleeing', 'risks', 'a', 'sniper', 'attack', '.', 'Government', 'troops', 'sometimes', 'escort', 'families', 'to', 'safety', ',', 'urging', 'them', 'to', 'hurry', 'through', 'streets', 'that', 'offer', 'little', 'cover', '.', 'Once', 'civilians', 'reach', 'a', 'safe', 'zone', ',', 'a', 'more', 'organized', 'system', 'is', 'in', 'place', 'to', 'transport', 'them', 'in', 'buses', 'and', 'on', 'the', 'backs', 'of', 'trucks', 'to', 'camps', 'for', 'displaced', 'people', '.', 'On', 'the', 'way', ',', 'they', 'receive', 'donations', 'of', 'food', 'and', 'water', '.', 'Iraqi', 'forces', 'have', 'been', 'told', 'to', 'destroy', 'evidence', 'of', 'the', 'Islamic', 'State', 'after', 'retaking', 'an', 'area', ',', 'but', 'signs', 'of', 'its', 'control', 'can', 'sometimes', 'be', 'difficult', 'to', 'efface', '.', 'Coalition', '  ', 'bombings', 'destroyed', 'an', 'Islamic', 'State', 'training', 'base', 'in', 'a', 'former', 'agricultural', 'college', 'in', 'the', 'town', 'of', 'Hamam', 'Alil', ',', 'below', '.', 'But', 'ISIS', 'writing', ',', 'which', 'states', 'the', 'name', 'of', 'a', 'unit', ',', 'remains', 'visible', 'on', 'the', 'wall', '.', 'Residents', ',', 'like', 'the', 'women', 'below', 'in', 'the', 'town', 'of', 'Bashiqa', ',', 'are', 'starting', 'to', 'return', 'to', 'areas', 'recently', 'freed', 'from', 'the', 'Islamic', 'State', 'to', 'learn', 'what', 'became', 'of', 'their', 'homes', 'and', 'to', 'gather', 'the', 'few', 'belongings', 'that', 'have', 'not', 'been', 'looted', 'or', 'destroyed', '.', 'The', 'woman', 'below', 'was', 'recovering', 'a', 'wedding', 'photo', 'from', 'the', 'rubble', 'in', 'Bashiqa', ',', 'northeast', 'of', 'Mosul', '.', 'The', 'town', ',', 'famous', 'for', 'its', 'olive', 'groves', ',', 'was', 'once', 'a', 'popular', 'day', 'trip', 'destination', 'from', 'the', 'city', 'and', 'had', 'a', 'large', 'population', 'of', 'Yazidis', '.', 'The', 'Islamic', 'State', '’s', 'control', 'of', 'Mosul', 'and', 'the', 'nearby', 'area', 'separated', 'families', 'for', 'long', 'stretches', '.', 'The', 'relatives', 'below', ',', 'reunited', 'on', 'the', 'side', 'of', 'the', 'road', ',', 'had', 'not', 'seen', 'one', 'another', 'in', 'two', 'years', '.', 'The', 'battle', 'for', 'Mosul', 'has', 'forced', 'many', 'tens', 'of', 'thousands', 'from', 'their', 'homes', ',', 'and', 'displaced', 'people', 'are', 'a', 'common', 'sight', 'along', 'the', 'roads', 'across', 'the', 'city', '.', 'Early', 'in', 'my', 'assignment', ',', 'I', 'accompanied', 'the', 'Iraqi', 'Army', 'to', 'this', 'neighborhood', 'below', ',', 'which', 'is', 'in', 'the', 'eastern', 'part', 'of', 'Mosul', ',', 'where', 'civilians', 'dared', 'to', 'venture', 'outside', 'as', 'government', 'troops', 'patrolled', '.', 'But', 'just', 'a', 'few', 'hours', 'after', 'this', 'photo', 'was', 'taken', ',', 'Islamic', 'State', 'fighters', 'attacked', 'an', 'Iraqi', 'Army', 'field', 'base', 'nearby', ',', 'killing', 'soldiers', ',', 'destroying', 'their', 'armored', 'vehicles', 'and', 'almost', 'immediately', 'posting', 'a', 'YouTube', 'video', 'triumphantly', 'showing', 'their', 'accomplishment', '.', 'After', 'that', ',', 'access', 'for', 'photographers', 'was', 'severely', 'curtailed', 'by', 'the', 'army', ',', 'and', 'journalists', 'quickly', 'went', 'from', 'having', 'a', '  ', 'seat', 'to', 'this', 'very', 'critical', 'battle', 'to', 'struggling', 'to', 'get', 'any', 'information', 'at', 'all', '.', 'Ambulances', 'carrying', 'those', 'gravely', 'injured', 'from', 'sniper', 'fire', ',', 'artillery', ',', 'mortars', 'and', 'mines', 'are', 'driven', 'out', 'of', 'Mosul', 'in', 'a', 'steady', 'flow', 'en', 'route', 'to', 'hospitals', 'in', 'nearby', 'Erbil', '.', 'When', 'fighting', 'is', 'heavy', ',', 'dozens', 'of', 'injured', 'people', 'arrive', 'at', 'the', 'hospitals', 'every', 'hour', '.', 'Field', 'hospitals', 'inside', 'Mosul', 'have', 'few', 'supplies', ',', 'so', 'victims', 'must', 'be', 'evacuated', 'to', 'where', 'they', 'can', 'be', 'treated', '.', 'Some', 'do', 'not', 'survive', 'the', 'journey', '.', 'The', 'emergency', 'vehicles', 'offered', 'me', 'one', 'of', 'the', 'most', 'reliable', 'indications', 'of', 'how', 'the', 'battle', 'was', 'going', ',', 'and', 'a', 'chance', 'to', 'document', 'the', 'results', '.', 'The', 'government', 'has', 'been', 'trying', 'to', 'consolidate', 'its', 'gains', ',', 'first', 'in', 'the', 'outer', 'provincial', 'villages', ',', 'and', 'now', 'in', 'suburbs', 'and', 'city', 'districts', 'ever', 'closer', 'to', 'the', 'city', 'center', '.', 'But', 'the', 'Islamic', 'State', 'had', 'a', 'long', 'time', 'to', 'prepare', 'for', 'this', 'battle', ',', 'and', 'its', 'fighters', 'are', 'skilled', 'and', '  ', 'taking', 'full', 'advantage', 'of', 'their', 'underground', 'network', 'of', 'tunnels', 'throughout', 'the', 'region', '.', 'A', 'long', 'and', 'difficult', 'fight', 'lies', 'ahead', 'for', 'Iraqi', 'forces', 'trying', 'to', 'liberate', 'Mosul', ',', 'like', 'this', 'lone', 'federal', 'policeman', 'on', 'a', 'still', 'very', 'dangerous', 'and', 'shifting', 'front', 'line', '.'], 'label': '0.0'}\n",
            "<torchtext.legacy.data.example.Example object at 0x7f4f24d30cd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma9ZPzeJcq79"
      },
      "source": [
        "Use the training data to create a vocabulary with a maximum amount of words. The larger the vocabulary, the larger the embedding layer in the RNN which will result in more learnable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0gqDkTycqUn"
      },
      "source": [
        "max_vocab_size = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=max_vocab_size)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur4pza5nc01X"
      },
      "source": [
        "Print the size of the vocabulary. The size will be the maximum of words plus two for the unk and pad tokens for words outside the vocabulary and padding, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp7_-t_Wc0UD",
        "outputId": "f5f6995d-8447-4ea9-80eb-c0216cdffcad"
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary: \"+str(len(TEXT.vocab)))\n",
        "print(\"Unique tokens in LABEL vocabulary: \"+str(len(LABEL.vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cexQzEd-fTqZ"
      },
      "source": [
        "Create an iterator that contains batches of tokenized text for the training, validation and testing later in the notebook. If run on Google Colab, turn on the GPU in Notebook settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03E-ViEfdVee"
      },
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data), \n",
        "    batch_sizes = (batch_size, batch_size, batch_size),\n",
        "    sort = False,\n",
        "    device = device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmHdkdsMfmu"
      },
      "source": [
        "Lets start introducing our first model. We start of with a simple RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51m2BBMjXMk"
      },
      "source": [
        "class Simple_RNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # The embedding layer takes the tokenized text input with the vocab\n",
        "        # size as dimension and outputs a vector with less elements of \n",
        "        # dimension embedding_dim.\n",
        "        self.embedding_layer = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        # Pass the output vector to an single-layer RNN.\n",
        "        self.rnn_layer = torch.nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # Pass the RNN output to a linear layer.\n",
        "        self.fc_layer = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.embedding_layer(text)\n",
        "        \n",
        "        output, hidden = self.rnn_layer(embedded)\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc_layer(hidden.squeeze(0))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xeSdSmPObs0"
      },
      "source": [
        "Now lets instantiate an model object.\n",
        "\n",
        "\n",
        "*   input_dim = the size of the vocabulary\n",
        "*   embedding_dim = the size of the vector that the embedding layer outputs.\n",
        "*   hiddem_dim = the size of the hidden states\n",
        "*   output_dim = one that quantifies the class probability i.e. being fake news or not.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsCJ-jcxN6BN"
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "\n",
        "model = Simple_RNN(input_dim, embedding_dim, hidden_dim, output_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdV5x0oyPQmf"
      },
      "source": [
        "Define the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjjoWl-WPNUF"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlebeOqUPVD2"
      },
      "source": [
        "Define the loss criterion that will be minimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKO47bLNPSOM"
      },
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kulei65APcnZ"
      },
      "source": [
        "Place the model and loss function on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfWnBd0MPZTd"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF9axKxyPooa"
      },
      "source": [
        "Define the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLScCPWRPfJD"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sACojeLvPzma"
      },
      "source": [
        "Define the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjO6xm1vPw1_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # Define placeholders for the loss and accuracy.\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # Set the model in train mode. This only affects \n",
        "    # dropout and batch norm layers.\n",
        "    model.train()\n",
        "    \n",
        "    # Loop over the batches.\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # Reset the gradient.\n",
        "        optimizer.zero_grad()\n",
        "            \n",
        "        # Make the predictions.\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        # Calculate the loss function.\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        # Calculate the accuracy.\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        # Do backpropagation.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Take a step in parameter space.\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Add the loss and accuracy to a total sum.\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    # Return the loss and accuracy averaged over the passed epochs.\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEKbkwoTP5rr"
      },
      "source": [
        "Define a loop to evaluate the model on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBC-JKSsP11O"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # Define placeholders for the loss and accuracy.\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # Set the model in train mode. This only affects \n",
        "    # dropout and batch norm layers.\n",
        "    model.eval()\n",
        "    \n",
        "    # Fix the parameters.\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Loop over the batches.\n",
        "        for batch in iterator:\n",
        "\n",
        "            # Make predictions.\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            # Calculate the loss.\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            # Calculate the accuracy.\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            # Add the loss and accuracy to a total sum.\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    # Return the loss and accuracy averaged over the passed epochs.\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlX7FBZQDe9"
      },
      "source": [
        "Define a function that will calculate the passed time for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1fB00UQBS9"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034z6RZwQMCg"
      },
      "source": [
        "Run the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldOq7gXWQJUy",
        "outputId": "c074d93c-d616-4f11-b948-545944672d96"
      },
      "source": [
        "n_epochs = 5\n",
        "\n",
        "# Loop over the data.\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, validation_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 28s\n",
            "\tTrain Loss: 0.696 | Train Acc: 50.02%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.46%\n",
            "Epoch: 02 | Epoch Time: 2m 28s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.79%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.46%\n",
            "Epoch: 03 | Epoch Time: 2m 27s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.93%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.48%\n",
            "Epoch: 04 | Epoch Time: 2m 27s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.19%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.48%\n",
            "Epoch: 05 | Epoch Time: 2m 27s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.22%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 49.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb0Wvk0liD2v"
      },
      "source": [
        "Test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhj7ijfLQNrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd77c020-4ff8-4fa6-db5d-3daf73ca69e7"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.693 | Test Acc: 49.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzs6NsVLhoXJ"
      },
      "source": [
        "The model shows no clear diminish in training or validation loss and results in an arbitrary testing accuracy of ~50%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUXGOGLWiiQ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}