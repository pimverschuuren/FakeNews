{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1-Qi7WXQiLhWGRPlNuce-vE0eWlrYvjw3",
      "authorship_tag": "ABX9TyMr5eyDb9OFQHP0yGJTHXtg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pimverschuuren/FakeNews/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2TD3_nvmveZ"
      },
      "source": [
        "This notebook implements a multilayered bidirectional LSTM model as a binary classifier that identifies text as fake or true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J69QdgJdJFMU",
        "outputId": "124f703c-1b55-47b0-ebea-beb600f02781"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K2hi54SWSh6"
      },
      "source": [
        "Create objects that will facilitate the tokenization of the textual data. Only include the columns that contain the text and the label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blwe0H6RWSH8"
      },
      "source": [
        "import torch\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "fields = [(None, None), (None, None), (None, None), ('text', TEXT),('label',LABEL)]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9vgytCrcHXH"
      },
      "source": [
        "Load the csv file into a pytorch dataset object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuVYoBuRBUfU"
      },
      "source": [
        "tab_data = data.TabularDataset(path = 'drive/MyDrive/Colab Notebooks/FakeNews/FakeNews.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX69x2TMce3T"
      },
      "source": [
        "Split the dataset into training, testing and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPTNjVTpE1Xc"
      },
      "source": [
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "train_data, test_data = tab_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n",
        "train_data, validation_data = train_data.split(split_ratio=0.75, random_state = random.seed(SEED))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdTEiy8Ffw3"
      },
      "source": [
        "Print the lengths of each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEmKYZm_BYgL",
        "outputId": "3ee09c06-9da3-4307-cef3-86aa0edb28ec"
      },
      "source": [
        "print(\"Number of training examples: \"+str(len(train_data)))\n",
        "print(\"Number of validation examples: \"+str(len(validation_data)))\n",
        "print(\"Number of testing examples: \"+str(len(test_data)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 10858\n",
            "Number of validation examples: 3619\n",
            "Number of testing examples: 6205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma9ZPzeJcq79"
      },
      "source": [
        "Use the training data to create a vocabulary with a maximum amount of words. Here we pass pretrained word vectors from the GloVe project that will supply initial weights for the embedding layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0gqDkTycqUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76349bb8-0100-4e7f-b1da-1fad09a15b4f"
      },
      "source": [
        "max_vocab_size = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=max_vocab_size, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:46, 5.19MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:19<00:00, 20577.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur4pza5nc01X"
      },
      "source": [
        "Print the size of the vocabulary. The size will be the maximum of words plus two for the unk and pad tokens for words outside the vocabulary and padding, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp7_-t_Wc0UD",
        "outputId": "42394560-ccec-46e8-a381-1db1619e9e75"
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary: \"+str(len(TEXT.vocab)))\n",
        "print(\"Unique tokens in LABEL vocabulary: \"+str(len(LABEL.vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cexQzEd-fTqZ"
      },
      "source": [
        "Create an iterator that contains batches of text for the training loop later in the notebook. If run on Google Colab, turn on the GPU in Notebook settings. Sort the texts within the batch on length so that the model learns the most in the beginning of the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03E-ViEfdVee"
      },
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data), \n",
        "    batch_sizes = (batch_size, batch_size, batch_size),\n",
        "    sort=False,\n",
        "    sort_within_batch=True,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    device = device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmHdkdsMfmu"
      },
      "source": [
        "Define a the LSTM model here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51m2BBMjXMk"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # The embedding layer takes the tokenized text input with the vocab\n",
        "        # size as dimension and outputs a vector with less elements of \n",
        "        # dimension embedding_dim. Include padding in this model to make\n",
        "        # the sentences equal in length.\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # Define the LSTM layers.\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        # The LSTM is bidirectional so 2 * hidden_dim outputs\n",
        "        # are passed to the linear layer.\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        # Include dropout for regularization.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        # Pack the text such that the hidden and cell state output of the \n",
        "        # the sequence is always of the non-padded token.\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "\n",
        "        # Pass the packed sequence to the LSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        \n",
        "        # Unpack the sequence.\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # Apply dropoout.\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xeSdSmPObs0"
      },
      "source": [
        "Now lets instantiate an model object.\n",
        "\n",
        "\n",
        "*   input_dim = the size of the vocabulary\n",
        "*   embedding_dim = the size of the vector that the embedding layer outputs.\n",
        "*   hiddem_dim = the size of the hidden states\n",
        "*   output_dim = one that quantifies the class probability i.e. being fake news or not.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsCJ-jcxN6BN"
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hiddem_dim = 256\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = LSTM(input_dim, \n",
        "            embedding_dim, \n",
        "            hiddem_dim, \n",
        "            output_dim, \n",
        "            n_layers, \n",
        "            bidirectional, \n",
        "            dropout, \n",
        "            pad_idx)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIrLjLQreTGw"
      },
      "source": [
        "Get the embedding layer weights from the pretrained GloVe word vectors. And set copy them to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6vcrFUmLYU1",
        "outputId": "334129e8-3f0d-4d73-f2e9-596fc786a026"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.1077,  0.1105,  0.5981,  ..., -0.8316,  0.4529,  0.0826],\n",
              "        ...,\n",
              "        [ 0.9178, -0.0447,  0.0414,  ...,  0.1672, -0.6644, -0.1582],\n",
              "        [-0.1308,  0.5205,  0.3289,  ..., -0.0279,  0.2165, -0.5381],\n",
              "        [-0.0693,  0.7122,  0.4987,  ..., -0.0958,  0.3595, -0.5329]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZP_nnKekbU"
      },
      "source": [
        "Set the weights in the embedding layer corresponding to the unk and pad tokens to zero. We do not want to learn anything from these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzncuG1hLfm5"
      },
      "source": [
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdV5x0oyPQmf"
      },
      "source": [
        "Define the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjjoWl-WPNUF"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlebeOqUPVD2"
      },
      "source": [
        "Define the loss criterion that will be minimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKO47bLNPSOM"
      },
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kulei65APcnZ"
      },
      "source": [
        "Place the model and loss function on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfWnBd0MPZTd"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF9axKxyPooa"
      },
      "source": [
        "Define the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLScCPWRPfJD"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sACojeLvPzma"
      },
      "source": [
        "Define the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjO6xm1vPw1_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # Define placeholders for the loss and accuracy.\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # Set the model in train mode. This only affects \n",
        "    # dropout and batch norm layers.\n",
        "    model.train()\n",
        "    \n",
        "    # Loop over the batches\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # Reset the gradient for each batch.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get the texts and text lengths.\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        # Make the predictions.\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        # Calculate the loss function.\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        # Calculate the accuracy.\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        # Perform back propagation.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Take a step in parameter space.\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Sum the loss and accuracy.\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    # Return the average loss and accuracy.\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEKbkwoTP5rr"
      },
      "source": [
        "Define a loop to evaluate the model on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBC-JKSsP11O"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # Define placeholders for the loss and accuracy.\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # Set the model in eval mode. This only affects \n",
        "    # dropout and batch norm layers.\n",
        "    model.eval()\n",
        "    \n",
        "    # Fix the parameters.\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Loop over the batches.\n",
        "        for batch in iterator:\n",
        "\n",
        "            # Get the texts and text lengths.\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            # Make predictions.\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            # Calculate the loss.\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            # Calculate the accuracy.\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            # Sum the loss and accuracy.\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    # Return the average loss and accuracy.\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlX7FBZQDe9"
      },
      "source": [
        "Define a function that will calculate the passed time for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1fB00UQBS9"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034z6RZwQMCg"
      },
      "source": [
        "Run the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldOq7gXWQJUy",
        "outputId": "2c72416b-86ae-418a-8cb1-64d591685f55"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, validation_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 18s\n",
            "\tTrain Loss: 0.355 | Train Acc: 83.46%\n",
            "\t Val. Loss: 0.097 |  Val. Acc: 96.03%\n",
            "Epoch: 02 | Epoch Time: 2m 19s\n",
            "\tTrain Loss: 0.111 | Train Acc: 95.69%\n",
            "\t Val. Loss: 0.048 |  Val. Acc: 98.30%\n",
            "Epoch: 03 | Epoch Time: 2m 19s\n",
            "\tTrain Loss: 0.113 | Train Acc: 95.78%\n",
            "\t Val. Loss: 0.035 |  Val. Acc: 98.79%\n",
            "Epoch: 04 | Epoch Time: 2m 23s\n",
            "\tTrain Loss: 0.038 | Train Acc: 98.73%\n",
            "\t Val. Loss: 0.019 |  Val. Acc: 99.42%\n",
            "Epoch: 05 | Epoch Time: 2m 24s\n",
            "\tTrain Loss: 0.023 | Train Acc: 99.24%\n",
            "\t Val. Loss: 0.017 |  Val. Acc: 99.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zLI698OKs1"
      },
      "source": [
        "Test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhj7ijfLQNrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3d69ac-9a0d-4224-888d-b7121b301ea5"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.018 | Test Acc: 99.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83TQAj9OMlL"
      },
      "source": [
        "The trained LSTM model shows a great accuracy of ~99% on the testing data."
      ]
    }
  ]
}